{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from statistics import mean\n",
        "from icecream import ic\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "sns.set()\n",
        "\n",
        "# The dataset is from https://archive.ics.uci.edu/ml/datasets/Balance+Scale\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('balance-scale.data', header=None,\n",
        "                names=['C','LW','LD','RW','RD'])\n",
        "df\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this file is to present (and generate) decision tree structures\n",
        "as to see why they are as effective with and without engineered features.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing dataset for training and testing under normal conditions\n",
        "\n",
        "X = df.loc[:, ['LW','LD','RW','RD']]\n",
        "y = df.loc[:, ['C']]\n",
        "X.columns.to_numpy()\n",
        "pd.unique(y.C.to_numpy())\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal dataset\n",
        "clf = DecisionTreeClassifier()\n",
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "print('With 10 cross_validation', 'Mean:', scores.mean(), scores.std())\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X, y)\n",
        "# tree.export_graphviz(clf, out_file=\"test1.dot\",\n",
        "#                      feature_names=X.columns.to_numpy(),\n",
        "#                      class_names=pd.unique(y.C.to_numpy()),\n",
        "#                      filled=True, rounded=True)\n",
        "l = [(name, importance) for name, importance in zip(['LW','LD','RW','RD'], clf.feature_importances_)]\n",
        "l.sort(reverse=True, key=lambda i: i[1])\n",
        "print('Feature importancen')\n",
        "for name, importance in l:\n",
        "    print(name, importance)\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce engineered weights\n",
        "left_array = df.loc[:, ['LW', 'LD']].to_numpy()\n",
        "calculations = [item[0] * item[1] for item in left_array]\n",
        "df['L_calc'] = calculations\n",
        "right_array = df.loc[:, ['RW', 'RD']].to_numpy()\n",
        "calculations = [item[0] * item[1] for item in right_array]\n",
        "df['R_calc'] = calculations\n",
        "X = df.loc[:, ['LW','LD','RW','RD','L_calc','R_calc']]\n",
        "y = df.loc[:, ['C']]\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with engineered features\n",
        "X = df.loc[:, ['LW','LD','RW','RD','L_calc','R_calc']]\n",
        "y = df.loc[:, ['C']]\n",
        "clf = DecisionTreeClassifier()\n",
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "print('With 10 cross_validation', 'Mean:', scores.mean(), scores.std())\n",
        "clf = DecisionTreeClassifier().fit(X, y)\n",
        "# tree.export_graphviz(clf, out_file=\"test2.dot\",\n",
        "#                      feature_names=X.columns.to_numpy(),\n",
        "#                      class_names=pd.unique(y.C.to_numpy()),\n",
        "#                      filled=True, rounded=True)\n",
        "l = [(name, importance) for name, importance in zip(['LW','LD','RW','RD','L_calc','R_calc'], clf.feature_importances_)]\n",
        "l.sort(reverse=True, key=lambda i: i[1])\n",
        "print('Feature importancen')\n",
        "for name, importance in l:\n",
        "    print(name, importance)\n",
        "\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with only engineered features\n",
        "X = df.loc[:, ['L_calc','R_calc']]\n",
        "y = df.loc[:, ['C']]\n",
        "clf = DecisionTreeClassifier()\n",
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "print('With 10 cross_validation', 'Mean:', scores.mean(), scores.std())\n",
        "clf = DecisionTreeClassifier().fit(X, y)\n",
        "# tree.export_graphviz(clf, out_file=\"test.dot\",\n",
        "#                      feature_names=X.columns.to_numpy(),\n",
        "#                      class_names=pd.unique(y.C.to_numpy()),\n",
        "#                      filled=True, rounded=True)\n",
        "l = [(name, importance) for name, importance in zip(['L_calc','R_calc'], clf.feature_importances_)]\n",
        "l.sort(reverse=True, key=lambda i: i[1])\n",
        "print('Feature importancen')\n",
        "for name, importance in l:\n",
        "    print(name, importance)\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Values of feature importances and the score:\n",
        "With 10 cross_validation Mean: 0.8860727086533539 0.09875318928663263\n",
        "Feature importancen\n",
        "L_calc 0.5175521558099616\n",
        "R_calc 0.48244784419003833\n",
        "LW 0.0\n",
        "LD 0.0\n",
        "RW 0.0\n",
        "RD 0.0\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updated Observations:\n",
        "- Ok so for tree with just calcaulted weights, the cross validation showd 97% of the accuracy for the dataset.\n",
        "    - It's really bizarre that it's roughly 10 percent more accurate than the tree with the original and engineered features despite having very identical tree structure!!! The only thing I deduce about this is that additional probably mess up accuracy of the tree when you give it to predict when a given tree doesn't use this sample. I suppose??? Need to ask somebody about this.\n",
        "\n",
        "Observations about the produced pdfs.\n",
        "- It looks like that tree with the calculated weights has a lot less horizontal spread compared to one without them.\n",
        "    - I believe that's reason the decision tree with the weights performs a lot better because it captured more **generality** thus not making the structure bigger.\n",
        "        - Algorithm Knowledge Update: The reason it managed to capture the mentioned \"generality\" is because the 'Gini' impurity calculation showed that the new features benefitted the tree due to samples being split better (i.e. a set of samples where most of them lean toward a particular class rather than having an almost equal number of them leaning towards three).\n",
        "    - Also, this definitely shows that the tree is not overfitted. This is because a symptoms of overfitting is when a tree is over-complex like the one without the calculated weights.\n",
        "- The tree with the weights prefers calculated weights over original features.\n",
        "    - This is probably because fewer features (i.e. the calculated weights) were more useful for generalisation rather than specific measures of the scales. The documentation mentioned that data with a large amount of features tend to overfit. Even though I only got four original features, the algorithm prefered the weight calculations nontheless. Need to generate trees with just a distance and weights and see how they develop.\n",
        "- Generating two tree structures that only used either weights or distances:\n",
        "    - After generating the trees, they looked identical to each other. I believe it's because the sample values were the same for both measurements that the tree just looked identical. Also performance was not worse or better: 60% accuracy for distance and 65% for weights.\n",
        "        - Algorithm Knowledge Update: I believe it's because of the huge amount of samples for 'left' and 'right' classes that the tree was very biased for them. This is because the 'Gini' impurity calculation proved that it couldn't go any lower due to being the 'best' split for samples. Thus, the leafs would be biased for two classes even though there were some 'Balanced' samples present in them.\n",
        "    - I noticed that the trees were very biased: There were no 'Balanced' class present at the leafs of the tree. It looks like the tree was not 'growing' completely without leafs representing the 'Balanced' class.\n",
        "    - Also, noticed that the trees had very weird leafs where it clould not decide whether it should have been 'Left' or 'Right' classes.\n",
        "        - Algorithm Knowledge Update: I believe the reason is that it couldn't split a given sample set at that point since its impurity was already low enough compared to calculated splits' ones. Thus, it produced a leaf where it just lingers in limbo of deciding whether it was right or left balanced.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with only weight-related features\n",
        "X = df.loc[:, ['LW','RW']]\n",
        "y = df.loc[:, ['C']]\n",
        "clf = DecisionTreeClassifier()\n",
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "print('With 10 cross_validation', 'Mean:', scores.mean(), scores.std())\n",
        "clf = DecisionTreeClassifier().fit(X, y)\n",
        "# tree.export_graphviz(clf, out_file=\"weights.dot\",\n",
        "#                      feature_names=X.columns.to_numpy(),\n",
        "#                      class_names=pd.unique(y.C.to_numpy()),\n",
        "#                      filled=True, rounded=True)\n",
        "l = [(name, importance) for name, importance in zip(['LW','LD'], clf.feature_importances_)]\n",
        "l.sort(reverse=True, key=lambda i: i[1])\n",
        "print('Feature importancen')\n",
        "for name, importance in l:\n",
        "    print(name, importance)\n",
        "\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with only distance-related features\n",
        "X = df.loc[:, ['RD','LD']]\n",
        "y = df.loc[:, ['C']]\n",
        "clf = DecisionTreeClassifier()\n",
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "print('With 10 cross_validation', 'Mean:', scores.mean(), scores.std())\n",
        "clf = DecisionTreeClassifier().fit(X, y)\n",
        "# tree.export_graphviz(clf, out_file=\"distance.dot\",\n",
        "#                      feature_names=X.columns.to_numpy(),\n",
        "#                      class_names=pd.unique(y.C.to_numpy()),\n",
        "#                      filled=True, rounded=True)\n",
        "l = [(name, importance) for name, importance in zip(['RW','RD'], clf.feature_importances_)]\n",
        "l.sort(reverse=True, key=lambda i: i[1])\n",
        "print('Feature importancen')\n",
        "for name, importance in l:\n",
        "    print(name, importance)\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with balanced number of classes\n",
        "TOTAL_NUMBER = df[df.C == 'B'].C.count()\n",
        "balanced = df[df.C == 'B']\n",
        "print(TOTAL_NUMBER)\n",
        "right_side = df[df.C == 'R'].sample(n=TOTAL_NUMBER)\n",
        "print(right_side.count())\n",
        "left_side = df[df.C == 'L'].sample(n=TOTAL_NUMBER)\n",
        "print(left_side.count())\n",
        "balanced_samples_df = pd.concat([balanced, left_side, right_side])\n",
        "balanced_samples_df.describe()\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing balanced tree without engineered features\n",
        "X = balanced_samples_df.loc[:, ['LW','LD','RW','RD']]\n",
        "y = balanced_samples_df.loc[:, ['C']]\n",
        "clf = DecisionTreeClassifier()\n",
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "print('With 10 cross_validation', 'Mean:', scores.mean(), scores.std())\n",
        "clf = DecisionTreeClassifier().fit(X, y)\n",
        "# tree.export_graphviz(clf, out_file=\"balanced_without_calc_weights.dot\",\n",
        "#                      feature_names=X.columns.to_numpy(),\n",
        "#                      class_names=pd.unique(y.C.to_numpy()),\n",
        "#                      filled=True, rounded=True)\n",
        "l = [(name, importance) for name, importance in zip(['LW','LD','RW','RD'], clf.feature_importances_)]\n",
        "l.sort(reverse=True, key=lambda i: i[1])\n",
        "print('Feature importancen')\n",
        "for name, importance in l:\n",
        "    print(name, importance)\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing balanced tree without engineered features\n",
        "X = balanced_samples_df.loc[:, ['LW','LD','RW','RD','L_calc','R_calc']]\n",
        "y = balanced_samples_df.loc[:, ['C']]\n",
        "clf = DecisionTreeClassifier()\n",
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "print('With 10 cross_validation', 'Mean:', scores.mean(), scores.std())\n",
        "clf = DecisionTreeClassifier().fit(X, y)\n",
        "# tree.export_graphviz(clf, out_file=\"balanced_with_calc_weights.dot\",\n",
        "#                      feature_names=X.columns.to_numpy(),\n",
        "#                      class_names=pd.unique(y.C.to_numpy()),\n",
        "#                      filled=True, rounded=True)\n",
        "l = [(name, importance) for name, importance in zip(['LW','LD','RW','RD','L_calc','R_calc'], clf.feature_importances_)]\n",
        "l.sort(reverse=True, key=lambda i: i[1])\n",
        "print('Feature importancen')\n",
        "for name, importance in l:\n",
        "    print(name, importance)\n",
        "\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations about the balanced distribution of the dataset.\n",
        "- For the balanced dataset without the calculated weights:\n",
        "    - Interestingly, the tree looked overcomplex. It shows that the tree was overfitting again due to nitpickiness of attributes. It performed poorly as expected (i.e. 53% accuracty with std of 12%).\n",
        "    - The reason like I said earlier was overfitting and its likely cause I think is overlearning the data to a point of making very intricate rules about each (or handful number of) sample because I think the data attributes couldn't be split further. I need to do some learning though about the algorithm.\n",
        "- For the balanced dataset without the calculated weights.\n",
        "    - It still looks kinda complex but it's not as big or wide as the previous tree. This is because the tree preferred the calculated weights over the original features and thus it managed to generalise the problem (i.e. the accuracy score is 65% with 14% standard deviation).\n",
        "    - Even though the structure looks kinda balanced on both sides, there's a subtree at the bottom where it goes 4 branches deep. I assume it is because there were some samples left at a point where it seemed it had converged that it was very difficult to equally split the samples based on a best attribute. Thus creating a tiny subtree.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do for figuring the inaccuracy of the model:\n",
        "1. Build a model with most of samples for all classes but leave out some samples of all classes for testing purposes:\n",
        "    - Check how many balanced have failed and passed.\n",
        "    - Check how many right side classses failed and passed.\n",
        "    - Check how many left side classses failed and passed.\n",
        "2. Build a model with balanced dataset but leave out some samples for the balanced class and the rest of the samples:\n",
        "    - Check how many balanced have failed and passed.\n",
        "    - Check how many right side classses failed and passed.\n",
        "    - Check how many left side classses failed and passed.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " UPDATE: Forget it since the tree can perform very well with the calculated weights alone.\n",
        " Make a dataset with a number of left out samples.\n",
        " The numbers are 10, 20, 30 for each class in every dataset.\n",
        "SEED = 1111\n",
        "\n",
        "for samples_num in [10, 20, 30]:\n",
        "    # Getting the general\n",
        "    X = non_sample_df.loc[:, ['LW','LD','RW','RD','L_calc','R_calc']]\n",
        "    y = non_sample_df.loc[:, ['C']]\n",
        "    clf = DecisionTreeClassifier()\n",
        "    scores = cross_val_score(clf, X, y, cv=10)\n",
        "    print('Cross validation with {0} samples:'.format(samples_num), 'Mean:', scores.mean(), 'Std:', scores.std())\n",
        "\n",
        "    b_rand_samples = df[df.C == 'B'].sample(n=samples_num, random_state=SEED)\n",
        "    l_rand_samples = df[df.C == 'L'].sample(n=samples_num, random_state=SEED)\n",
        "    r_rand_samples = df[df.C == 'R'].sample(n=samples_num, random_state=SEED)\n",
        "    indices_list = list(b_rand_samples.index.to_numpy()) + list(l_rand_samples.index.to_numpy()) + list(r_rand_samples.index.to_numpy())\n",
        "    print('Sum of indicies list is', len(indices_list))\n",
        "    non_sample_indicies = set(df.index.to_numpy()) - set(indices_list)\n",
        "    print('Sum of non_sample_indicies set is', len(non_sample_indicies))\n",
        "    non_sample_df = df.iloc[list(non_sample_indicies)]\n",
        "    print(\"Non sample df head:\\n\", non_sample_df.head())\n",
        "\n",
        "    X = non_sample_df.loc[:, ['LW','LD','RW','RD','L_calc','R_calc']]\n",
        "    y = non_sample_df.loc[:, ['C']]\n",
        "    clf = DecisionTreeClassifier()\n",
        "    scores = cross_val_score(clf, X, y, cv=10)\n",
        "    print('Cross validation with {0} samples:'.format(samples_num), 'Mean:', scores.mean(), 'Std:', scores.std())\n",
        "    clf = DecisionTreeClassifier().fit(X, y)\n",
        "    # Need to write cross scores\n",
        "    # Uncomment once the code is ready\n",
        "    # tree.export_graphviz(clf, out_file=\"balanced_with_calc_weights.dot\",\n",
        "    #                      feature_names=X.columns.to_numpy(),\n",
        "    #                      class_names=pd.unique(y.C.to_numpy()),\n",
        "    #                      filled=True, rounded=True)\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform to cross_val_score for trees with all features.\n",
        "from sklearn.model_selection import cross_validate\n",
        "X = df.loc[:, ['LW','LD','RW','RD','L_calc','R_calc']]\n",
        "y = df.loc[:, ['C']]\n",
        "clf = DecisionTreeClassifier()\n",
        "validations = cross_validate(clf, X, y, cv=10, return_estimator=True, return_train_score=True)\n",
        "# print('With 10 cross_validation with all features', 'Mean:', scores.mean(), scores.std())\n",
        "list(validations.keys())\n",
        "type(validations['estimator'])\n",
        "print(validations['test_score'])\n",
        "print(mean(validations['test_score']))\n",
        "\n",
        "\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing balanced tree without engineered features\n",
        "X = df.loc[:, ['L_calc','R_calc']]\n",
        "y = df.loc[:, ['C']]\n",
        "clf = DecisionTreeClassifier()\n",
        "validations = cross_validate(clf, X, y, cv=10, return_estimator=True, return_train_score=True)\n",
        "# print('With 10 cross_validation with only engineered features', 'Mean:', scores.mean(), scores.std())\n",
        "list(validations.keys())\n",
        "type(validations['estimator'])\n",
        "print(validations['test_score'])\n",
        "print(mean(validations['test_score']))\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "ALL_FEATURES_COLUMNS = ['LW','LD','RW','RD','L_calc','R_calc']\n",
        "X = df.loc[:, ALL_FEATURES_COLUMNS]\n",
        "y = df.loc[:, ['C']]\n",
        "# Creating estimators with all the features\n",
        "sKf = StratifiedKFold(n_splits=10)\n",
        "# Format: [{'fitted_estimator', 'mean_score', 'test_split': {'X_test', 'y_test'}, 'train_split': {'X_train', 'y_train'}}]\n",
        "all_features_cv_list = []\n",
        "indices_list = []\n",
        "for train_index, test_index in sKf.split(X, y):\n",
        "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
        "    X_train, X_test = X.to_numpy()[train_index], X.to_numpy()[test_index]\n",
        "    y_train, y_test = y.to_numpy()[train_index], y.to_numpy()[test_index]\n",
        "    # print(X_train[0:5])\n",
        "    # print(y_train[0:5])\n",
        "    # print(X_test[0:5])\n",
        "    # print(y_test[0:5])\n",
        "    clf = DecisionTreeClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "    mean_score = clf.score(X_test, y_test)\n",
        "    print('MEAN:', mean_score)\n",
        "    d = {\n",
        "        'mean_score': mean_score,\n",
        "        'fitted_estimator': clf,\n",
        "        'test_split': {'X_test': X_test, 'y_test': y_test},\n",
        "        'train_split': {'X_train': X_train, 'y_train': y_train}\n",
        "    }\n",
        "    all_features_cv_list.append(d)\n",
        "    indices_list.append((train_index, test_index))\n",
        "mean([d['mean_score'] for d in all_features_cv_list])\n",
        "\n",
        "# It was one hell of a learning experience to realise that for multi-class problems 'StratifiedKFold' is used to split the data.\n",
        "\n",
        "# Otherwise, I ended up believing that my estimators were 'unicorns' due to their high accuracies that averaged 95% of the time.\n",
        "\n",
        "# Creating estimators with engineered features only!\n",
        "ENGINEERED_FEATURES_COLUMNS = ['L_calc','R_calc']\n",
        "X = df.loc[:, ENGINEERED_FEATURES_COLUMNS]\n",
        "y = df.loc[:, ['C']]\n",
        "# Format: [{'fitted_estimator', 'mean_score', 'test_split': {'X_test', 'y_test'}, 'train_split': {'X_train', 'y_train'}}]\n",
        "engineered_features_cv_list = []\n",
        "for train_index, test_index in indices_list:\n",
        "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
        "    X_train, X_test = X.to_numpy()[train_index], X.to_numpy()[test_index]\n",
        "    y_train, y_test = y.to_numpy()[train_index], y.to_numpy()[test_index]\n",
        "    # print(X_train[0:5])\n",
        "    # print(y_train[0:5])\n",
        "    # print(X_test[0:5])\n",
        "    # print(y_test[0:5])\n",
        "    clf = DecisionTreeClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "    mean_score = clf.score(X_test, y_test)\n",
        "    print('MEAN:', mean_score)\n",
        "    d = {\n",
        "        'mean_score': mean_score,\n",
        "        'fitted_estimator': clf,\n",
        "        'test_split': {'X_test': X_test, 'y_test': y_test},\n",
        "        'train_split': {'X_train': X_train, 'y_train': y_train}\n",
        "    }\n",
        "    engineered_features_cv_list.append(d)\n",
        "mean([d['mean_score'] for d in engineered_features_cv_list])\n",
        "\n",
        "# Function for producing dot and pdfs files\n",
        "def make_pdf(name, index, clf, columns, uniq_names):\n",
        "    TEMPLATE = '{name}_{index}'\n",
        "    dot_name = TEMPLATE.format(name=name, index=index) + \".dot\"\n",
        "    tree.export_graphviz(clf, out_file='produced_pdfs/'+dot_name,\n",
        "                         # feature_names=X.columns.to_numpy(),\n",
        "                         feature_names=columns,\n",
        "                         class_names=uniq_names,\n",
        "                         filled=True, rounded=True)\n",
        "    os.system('dot -Tps produced_pdfs/{0}.dot -o produced_pdfs/{0}.pdf'.format(TEMPLATE.format(name=name, index=index)))\n",
        "\n",
        "# Calculating performance differences between trained classifiers\n",
        "UNIQUE_CLASS_NAMES = pd.unique(y.C.to_numpy())\n",
        "\n",
        "performance_diff_list = []\n",
        "engineered_perf_list = []\n",
        "all_f_perf_list = []\n",
        "counter_list = []\n",
        "for index, dict_tuple in enumerate(zip(all_features_cv_list, engineered_features_cv_list)):\n",
        "    all_f_dict, engineered_f_dict = dict_tuple\n",
        "    # It's expected for engineered features to perform better thus substracting from its performance score\n",
        "    performance_diff_list.append(engineered_f_dict['mean_score'] - all_f_dict['mean_score'])\n",
        "    engineered_perf_list.append(engineered_f_dict['mean_score'])\n",
        "    all_f_perf_list.append(all_f_dict['mean_score'])\n",
        "    counter_list.append(index)\n",
        "    # Creating pdfs!\n",
        "    # make_pdf('all_f', index, all_f_dict['fitted_estimator'], ALL_FEATURES_COLUMNS, UNIQUE_CLASS_NAMES)\n",
        "    # make_pdf('engi_f', index, engineered_f_dict['fitted_estimator'], ENGINEERED_FEATURES_COLUMNS, UNIQUE_CLASS_NAMES)\n",
        "\n",
        "# os.system('rm produced_pdfs/*.dot')\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'cv_iter': pd.Series(counter_list),\n",
        "    'engineered_perf': pd.Series(engineered_perf_list),\n",
        "    'all_perf': pd.Series(all_f_perf_list),\n",
        "    'perf_diff': pd.Series(performance_diff_list),\n",
        "})\n",
        "\n",
        "data\n",
        "\n",
        "'''\n",
        "# Results at the time of generating the files.\n",
        "# If you run this notebook/file the results may vary slightly.\n",
        "cv_iter\tengineered_perf\tall_perf\tperf_diff\n",
        "0\t0\t0.936508\t0.888889\t0.047619\n",
        "1\t1\t1.000000\t0.968254\t0.031746\n",
        "2\t2\t0.968254\t0.936508\t0.031746\n",
        "3\t3\t1.000000\t1.000000\t0.000000\n",
        "4\t4\t1.000000\t0.873016\t0.126984\n",
        "5\t5\t0.983871\t0.758065\t0.225806\n",
        "6\t6\t1.000000\t1.000000\t0.000000\n",
        "7\t7\t0.983871\t0.693548\t0.290323\n",
        "8\t8\t0.967742\t0.870968\t0.096774\n",
        "9\t9\t0.935484\t0.854839\t0.080645\n",
        "'''\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_stuff(all_features_cv_list, engineered_features_cv_list, data):\n",
        "    # (Hopefully) saving the current state of the classifiers and test and train indicies to a file\n",
        "    with open('saves/all_features_cv_list.pickle', 'wb') as file_obj:\n",
        "        pickle.dump(all_features_cv_list, file_obj)\n",
        "    with open('saves/engineered_features_cv_list.pickle', 'wb') as file_obj:\n",
        "        pickle.dump(engineered_features_cv_list, file_obj)\n",
        "    data.to_pickle('saves/data_dataframe.pickle')\n",
        "\n",
        "def get_stuff():\n",
        "    with open('saves/all_features_cv_list.pickle', 'rb') as file_obj:\n",
        "        resurectted = pickle.load(file_obj)\n",
        "    with open('saves/engineered_features_cv_list.pickle', 'rb') as file_obj:\n",
        "        resurectted_2 = pickle.load(file_obj)\n",
        "    resurect_data = pd.read_pickle('saves/data_dataframe.pickle')\n",
        "    return resurectted, resurectted_2, resurect_data\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting stuff back from saves files\n",
        "all_features_cv_list, engineered_features_cv_list, data = get_stuff()\n",
        "data\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def display_info(index, all_f_dict, engineered_f_dict):\n",
        "    '''Function to display informative values about a CV iteration.\n",
        "    TO-DO:\n",
        "    - Displaying a list of failed samples for each classifier\n",
        "    '''\n",
        "    print('Iteration_CV:', index)\n",
        "    print('All_features_score:', all_f_dict['mean_score'])\n",
        "    print('Engineered_features_score:', engineered_f_dict['mean_score'])\n",
        "    print('Perf_diff:', engineered_f_dict['mean_score'] - all_f_dict['mean_score'])\n",
        "\n",
        "    # Since the test and training data splits are the same for engineered and features\n",
        "    print('Training_data_total', len(all_f_dict['train_split']['X_train']), 'Testing_data_total', len(all_f_dict['test_split']['X_test']))\n",
        "    # But it won't escape checking ;)\n",
        "    # 4, 5 indicies for the all features since that's the calcualted weights.\n",
        "    all_f_samples = [[sample[4], sample[5]] for sample in all_f_dict['train_split']['X_train']]\n",
        "    assert all([all(l) for l in all_f_samples == engineered_f_dict['train_split']['X_train']])\n",
        "    assert all([all(l) for l in all_f_dict['train_split']['y_train'] == engineered_f_dict['train_split']['y_train']])\n",
        "    all_f_samples = [[sample[4], sample[5]] for sample in all_f_dict['test_split']['X_test']]\n",
        "    assert all([all(l) for l in all_f_samples == engineered_f_dict['test_split']['X_test']])\n",
        "    assert all([all(l) for l in all_f_dict['test_split']['y_test'] == engineered_f_dict['test_split']['y_test']])\n",
        "\n",
        "    # Printing percentages of classes\n",
        "    train_classes_counter = Counter([i[0] for i in all_f_dict['train_split']['y_train']])\n",
        "    test_classes_counter = Counter([i[0] for i in all_f_dict['test_split']['y_test']])\n",
        "    total_train_samples = len(all_f_dict['train_split']['y_train'])\n",
        "    total_test_samples = len(all_f_dict['test_split']['y_test'])\n",
        "    train_sample_percentages = [(sample_class, round((count/total_train_samples)*100, 2)) for sample_class, count in train_classes_counter.most_common()]\n",
        "    test_sample_percentages = [(sample_class, round((count/total_test_samples)*100, 2)) for sample_class, count in test_classes_counter.most_common()]\n",
        "    print('train_samples_classes_percentages:', train_sample_percentages)\n",
        "    print('test_samples_classes_percentages:', test_sample_percentages)\n",
        "\n",
        "    # Print successful and failed samples for all features classifier\n",
        "    test_split_X = all_f_dict['test_split']['X_test']\n",
        "    test_split_y = all_f_dict['test_split']['y_test']\n",
        "    all_f_clf = all_f_dict['fitted_estimator']\n",
        "    predicted_classes = all_f_clf.predict(test_split_X)\n",
        "    # print('all_f_predicted_classes:', predicted_classes)\n",
        "\n",
        "    # Getting separate attributes for each sample for all features\n",
        "    features_dict = {}\n",
        "    for index, feature in enumerate(ALL_FEATURES_COLUMNS):\n",
        "        features_dict[feature] = [a[index] for a in test_split_X]\n",
        "\n",
        "    all_f_for_df_dict = {\n",
        "        'returned_class': pd.Series(predicted_classes),\n",
        "        'expected_class': pd.Series([i[0] for i in test_split_y])\n",
        "    }\n",
        "    all_f_for_df_dict = {**all_f_for_df_dict, **features_dict}\n",
        "    all_f_df = pd.DataFrame(all_f_for_df_dict)\n",
        "    matches = [row['returned_class'] == row['expected_class'] for index, row in all_f_df.iterrows()]\n",
        "    all_f_df['matched'] = pd.Series(matches)\n",
        "    print('All Features table')\n",
        "    print(all_f_df.groupby(['expected_class', 'matched']).count())\n",
        "\n",
        "    # Print successful and failed samples for engineered features classifier\n",
        "    test_split_X = engineered_f_dict['test_split']['X_test']\n",
        "    test_split_y = engineered_f_dict['test_split']['y_test']\n",
        "    engineered_f_clf = engineered_f_dict['fitted_estimator']\n",
        "    predicted_classes = engineered_f_clf.predict(test_split_X)\n",
        "    # print('engineered_predicted_classes:', predicted_classes)\n",
        "\n",
        "    # Getting separate attributes for each sample for all features\n",
        "    features_dict = {}\n",
        "    for index, feature in enumerate(ENGINEERED_FEATURES_COLUMNS):\n",
        "        features_dict[feature] = [a[index] for a in test_split_X]\n",
        "\n",
        "    engineered_f_for_df_dict = {\n",
        "        'returned_class': pd.Series(predicted_classes),\n",
        "        'expected_class': pd.Series([i[0] for i in test_split_y])\n",
        "    }\n",
        "    engineered_f_for_df_dict = {**engineered_f_for_df_dict, **features_dict}\n",
        "    engineered_f_df = pd.DataFrame(engineered_f_for_df_dict)\n",
        "    matches = [row['returned_class'] == row['expected_class'] for index, row in engineered_f_df.iterrows()]\n",
        "    engineered_f_df['matched'] = pd.Series(matches)\n",
        "    print('Engineered Features table')\n",
        "    print(engineered_f_df.groupby(['expected_class', 'matched']).count())\n",
        "\n",
        "    # Create a dataframe where samples from all_f_df failed as well as succeed in the engineered_f_df\n",
        "    row_index_list = []\n",
        "    for index, tuples in enumerate(zip(all_f_df.itertuples(), engineered_f_df.itertuples())):\n",
        "        all_f_row, engineered_f_row = tuples[0], tuples[1]\n",
        "        if not all_f_row.matched and engineered_f_row.matched:\n",
        "            row_index_list.append(index)\n",
        "\n",
        "    valid_samples_df = all_f_df.iloc[row_index_list]\n",
        "    print(\"Counts\")\n",
        "    print(valid_samples_df.count())\n",
        "    return valid_samples_df\n",
        "\n",
        "def list_decision_travel_nodes(all_f_dict, LW, LD, RW, RD, L_calc, R_calc):\n",
        "    '''List nodes where a sample traversed through whilst predicted a sample'''\n",
        "    # Print successful and failed samples for all features classifier\n",
        "    classifier = all_f_dict['fitted_estimator']\n",
        "    X_test = np.ndarray([LW, LD, RW, RD, L_calc, R_calc])\n",
        "    X_test = [X_test]\n",
        "\n",
        "    ic(dir(classifier))\n",
        "    ic(classifier.n_features_)\n",
        "    node_indicator = classifier.decision_path(X_test)\n",
        "    leave_id = classifier.apply(X_test)\n",
        "\n",
        "    sample_id = 0 # Since I only provided one sample, the sample id should be 0 by default\n",
        "    node_index = node_indicator.indicies[node_indicator.indptr[sample_id],\n",
        "                                         node_indicator.indptr[sample_id + 1]]\n",
        "\n",
        "    print(\"Decision path for the provided paramters:\")\n",
        "    for node_id in node_index:\n",
        "        if leave_id[sample_id] == node_id:\n",
        "            continue\n",
        "\n",
        "        if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):\n",
        "            threshold_sign = \"<=\"\n",
        "        else:\n",
        "            threshold_sign = \">\"\n",
        "\n",
        "        print(\"decision id node %s : (X_test[%s, %s] (= %s) %s %s)\"\n",
        "              % (node_id,\n",
        "                 sample_id,\n",
        "                 feature[node_id],\n",
        "                 X_test[sample_id, feature[node_id]],\n",
        "                 threshold_sign,\n",
        "                 threshold[node_id]))\n",
        "    # Unfortunately will have to stop implementing the path printer since I encountered an error that can't problem solve.\n",
        "    # Going to explore the tree manually to see why 7th cross validation has got majority of samples failing\n",
        "\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trees\n",
        "# Just a test code. Ignore.\n",
        "display_info(7, all_features_cv_list[7], engineered_features_cv_list[7])\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trees that have zero difference in performance\n",
        "display_info(3, all_features_cv_list[3], engineered_features_cv_list[3])\n",
        "display_info(6, all_features_cv_list[6], engineered_features_cv_list[6])\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, so this is not suprising since the trees (almost) closely identically to one another so there's zero difference in performance.\n",
        "\n",
        "3 and 3 iterations all_f: the tree structures look very identical. Except for one leftist node. One tree has used L_calc whereas the other used R_calc. I believe this is because at that node, the attributes would have the same Gini value and thus it would've randomly selected the attribute.\n",
        "Despite that, leaves are the same between trees.\n",
        "\n",
        "6 and 6 iterations engineered_f:\n",
        "Trees look very idnetical.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trees with roughly 10 performance difference on average\n",
        "# 4, 8, 9\n",
        "display_info(4, all_features_cv_list[4], engineered_features_cv_list[4])\n",
        "display_info(8, all_features_cv_list[8], engineered_features_cv_list[8])\n",
        "display_info(9, all_features_cv_list[9], engineered_features_cv_list[9])\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration 4:\n",
        "    - It looks like the trees look very identical, but the all_f tree had used an original attribute right in the centre of the tree. Possibly because of it the all_f tree under performed. However, need to implement the diagnostic function to see where the path travel.\n",
        "\n",
        "Iteration 8:\n",
        "    - Trees look almost identical, but again, an original feature introduced an additional depth to the all_f tree compared to engineered_f one.\n",
        "\n",
        "Iteration 9:\n",
        "    - Trees look identical, but all_f's three nodes use original features to split the samples. However, it does look like leaves are the same between the trees.\n",
        "\n",
        "In conclusion for the specified iterations, the trees almost look alike, but the original features either alter a structure of the tree a bit or a node, which would usually use an engineered node.\n",
        "\n",
        "My assumption is that the reason the original features are used is because at that particular node had either the same amount of samples that the splits were so similar that the attribute was chosen at random *or* the split was a bit better than the engineeered ones.\n",
        "\n",
        "And that's why there were a handful of mismatches for each class in iterations since a particular set of samples would have gone a path that wasn't generalised/trained on it properly.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trees with roughly 3 to 4 percent different in performance\n",
        "# 0, 1, 2\n",
        "display_info(0, all_features_cv_list[0], engineered_features_cv_list[0])\n",
        "display_info(1, all_features_cv_list[1], engineered_features_cv_list[1])\n",
        "display_info(2, all_features_cv_list[2], engineered_features_cv_list[2])\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trees with bigger performance differences.\n",
        "# 5, 7\n",
        "display_info(5, all_features_cv_list[5], engineered_features_cv_list[5])\n",
        "display_info(7, all_features_cv_list[7], engineered_features_cv_list[7])\n",
        "\n",
        "#"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "outputExpanded": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Trees with the worst performance\n",
        "Iteration 5\n",
        "    - The trees look very familiar but there's one path that stands out. In the all features tree, on a second node ('L_calc' <= 13.5), a 'LD' feature was selected and as a result the structure of the path looks different to the path in the engineered tree: the feature gave itself an extra two depths and it had some leaves along the way whereas the engineered tree had a balanced subtree.\n",
        "        - Ok this is weird: for the right most node (in all features tree), the gini confidence is the same as that of the engineered branch. I think the value for the gini confidence was still different because the values are rounded and therefore the values would be very close to between each other but still different. (will need to math for that one)\n",
        "        - Also, by looking at the failed samples (which engineered_f tree succeeded), it looks like for samples that have only 2 for 'LW' and 5 'LD' features made the trained model fail by making it think that it's balanced even if the other 4 features indicated that it was leaning on the right.\n",
        "            - Note that it's the 10 calculated weight.\n",
        "            - By looking at the tree, I followed that the samples that got the class wrong would have to go through (LD <= 4.5) condition on right most path of the tree from the second node of (L_calc <= 13.5).\n",
        "        - For other failed samples,\n",
        "            - samples that had balanced labels had a thing where each two original feature of each side were the same and either in a repeating or mirrored pattern (look at the table with index of 30, 31). My guess is that a particular node that would get one of the sample would lead them in wrong a path because the mathematical rule dimmed it 'best' to split the dataset at that time.\n",
        "            - Similar issue hapens for the 38 and 45 index where it exted left leaning side rather than balanced.\n",
        "Iteration 7\n",
        "    - What's interesting is that half of the left side classes samples fail completely in comparison to the right side (even though the balanced class has 50/50 success/failire rate as well).\n",
        "    - The trees again look identiacal, but the all_f tree has a node that disticly altered the shape of a subtree, especially for the node under the second node on hte right side (i.e. R_calc <= 13.5). The subtree uses an original attribute to split the samples and it introduces an additional node that would seperate an a subtree strucutre very similar to the engineered_f subtree under the same node I mentioned.\n",
        "    - There's another node that uses the the original attribute but the leaves under it seem the same between the trees.\n",
        "    - Looking at the produced table of failed samples:\n",
        "        - It looks like that samples (from 40 to 54 indicies) share three features in common: LW=5, LD=2 and L_calc=10. These samples expected a left class but got balanced class instead.\n",
        "            - Ok, by looking at the tree and the node that I noticed that looked out of place (i.e. R_calc <= 13.5) and its following nodes of (LD <= 2.5) and (R_calc <=11.0) will make the samples mentioned into the balanced label even if it's incorrectly mathematically. My speculation is that the tree got overtrained on a particular set of samples that made the subtree with aforementioned nodes.\n",
        "\n",
        "In conclusion, it seems original attributes, which have been selected as the best sample splitting attribute according to Gini value, affect the structure of the trees in such a way that tested sample fail the most. Therefore, I need to confirm that these attributes affect the classification prediciton of these samples by looking at the path that attributes take.\n",
        "\n",
        "*Additional* conclusion: I noticed something really weird between two trees: it's not only the original feature that ruined the subree creation as to create some sort of overfitting, it's the node that were produced by such feature had either a node or a subtree with 1 or 2 depths that had very few samples at the leaves. In other words, even though the tree was trained to accommodate these few examples, it made the tree fail at getting that 'generalisation' it needs to be accurate... ehm see distinction between two classes."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "argv": [
        "/home/pavel/.miniconda3/envs/scales/bin/python",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}